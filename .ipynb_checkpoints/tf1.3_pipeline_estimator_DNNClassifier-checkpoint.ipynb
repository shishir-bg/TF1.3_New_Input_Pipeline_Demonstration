{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Estimators + Dataset(new input pipeline) Example - Categorize Iris flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "if sys.version_info < (3, 0, 0):\n",
    "    from urllib import urlopen\n",
    "else:\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "# Check that we have correct TensorFlow version installed\n",
    "tf_version = tf.__version__\n",
    "print(\"TensorFlow version: {}\".format(tf_version))\n",
    "assert \"1.3\" <= tf_version, \"TensorFlow r1.3 or later is needed\"\n",
    "\n",
    "# set Path\n",
    "PATH = \"/Users/shishir/Documents/botconnect Playground/TF1.3_input_pipeline/\"\n",
    "\n",
    "# Fetch and store Training and Test dataset files\n",
    "PATH_DATASET = PATH + os.sep + \"dataset\"\n",
    "FILE_TRAIN = PATH_DATASET + os.sep + \"iris_training.csv\"\n",
    "FILE_TEST = PATH_DATASET + os.sep + \"iris_test.csv\"\n",
    "URL_TRAIN = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "URL_TEST = \"http://download.tensorflow.org/data/iris_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downloadDataset(url, file):\n",
    "    if not os.path.exists(PATH_DATASET):\n",
    "        os.makedirs(PATH_DATASET)\n",
    "    if not os.path.exists(file):\n",
    "        data = urlopen(url).read()\n",
    "        with open(file, \"wb\") as f:\n",
    "            f.write(data)\n",
    "            f.close()\n",
    "downloadDataset(URL_TRAIN, FILE_TRAIN)\n",
    "downloadDataset(URL_TEST, FILE_TEST)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The CSV features in our training & test data\n",
    "feature_names = [\n",
    "    'SepalLength',\n",
    "    'SepalWidth',\n",
    "    'PetalLength',\n",
    "    'PetalWidth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#New Input Pipeline API\n",
    "\n",
    "def input_pipeline(file_path, perform_shuffle=False, repeat_count=1):\n",
    "    def decode_csv(line):\n",
    "        parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]])\n",
    "        label = parsed_line[-1:]  # Last element is the label\n",
    "        del parsed_line[-1]  # Delete last element\n",
    "        features = parsed_line  # Everything but last elements are the features\n",
    "        d = dict(zip(feature_names, features)), label\n",
    "        return d\n",
    "\n",
    "    dataset = (tf.contrib.data.TextLineDataset(file_path)  # Read text file using new API\n",
    "               .skip(1)  # Skip header row\n",
    "               .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
    "    if perform_shuffle:\n",
    "        # Randomizes input using a window of 256 elements (read into memory)\n",
    "        dataset = dataset.shuffle(buffer_size=256)\n",
    "    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\n",
    "    dataset = dataset.batch(32)  # Batch size to use\n",
    "    batch_features, batch_labels = dataset.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    \n",
    "    #Allowed structure had changed. Now x = make_iterator\n",
    "    #x.get_next() returns an error\n",
    "    \n",
    "    \n",
    "    \n",
    "    return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'PetalLength': array([ 1.20000005,  6.9000001 ,  1.29999995,  4.        ,  5.5999999 ,\n",
      "        5.5999999 ,  1.5       ,  3.9000001 ,  6.30000019,  6.5999999 ,\n",
      "        5.19999981,  1.60000002,  4.5999999 ,  1.60000002,  3.5999999 ,\n",
      "        5.5999999 ,  1.5       ,  5.80000019,  4.5999999 ,  4.4000001 ,\n",
      "        4.19999981,  1.39999998,  6.69999981,  1.29999995,  1.        ,\n",
      "        1.60000002,  1.39999998,  5.5       ,  1.29999995,  4.4000001 ,\n",
      "        5.5999999 ,  1.5       ], dtype=float32), 'PetalWidth': array([ 0.2       ,  2.29999995,  0.2       ,  1.29999995,  1.39999998,\n",
      "        2.4000001 ,  0.1       ,  1.10000002,  1.79999995,  2.0999999 ,\n",
      "        2.29999995,  0.2       ,  1.5       ,  0.2       ,  1.29999995,\n",
      "        2.20000005,  0.40000001,  2.20000005,  1.29999995,  1.29999995,\n",
      "        1.29999995,  0.2       ,  2.20000005,  0.2       ,  0.2       ,\n",
      "        0.40000001,  0.2       ,  1.79999995,  0.40000001,  1.39999998,\n",
      "        2.4000001 ,  0.2       ], dtype=float32), 'SepalLength': array([ 5.        ,  7.69999981,  4.4000001 ,  6.0999999 ,  6.0999999 ,\n",
      "        6.69999981,  4.9000001 ,  5.5999999 ,  7.30000019,  7.5999999 ,\n",
      "        6.69999981,  5.0999999 ,  6.5       ,  4.80000019,  5.5999999 ,\n",
      "        6.4000001 ,  5.69999981,  6.5       ,  6.5999999 ,  6.30000019,\n",
      "        5.69999981,  4.4000001 ,  7.69999981,  5.5       ,  4.5999999 ,\n",
      "        5.        ,  5.        ,  6.5       ,  5.4000001 ,  6.5999999 ,\n",
      "        6.30000019,  5.4000001 ], dtype=float32), 'SepalWidth': array([ 3.20000005,  2.5999999 ,  3.20000005,  2.79999995,  2.5999999 ,\n",
      "        3.0999999 ,  3.0999999 ,  2.5       ,  2.9000001 ,  3.        ,\n",
      "        3.        ,  3.79999995,  2.79999995,  3.0999999 ,  2.9000001 ,\n",
      "        2.79999995,  4.4000001 ,  3.        ,  2.9000001 ,  2.29999995,\n",
      "        2.9000001 ,  2.9000001 ,  3.79999995,  3.5       ,  3.5999999 ,\n",
      "        3.4000001 ,  3.5999999 ,  3.        ,  3.9000001 ,  3.        ,\n",
      "        3.4000001 ,  3.70000005], dtype=float32)}, array([[0],\n",
      "       [2],\n",
      "       [0],\n",
      "       [1],\n",
      "       [2],\n",
      "       [2],\n",
      "       [0],\n",
      "       [1],\n",
      "       [2],\n",
      "       [2],\n",
      "       [2],\n",
      "       [0],\n",
      "       [1],\n",
      "       [0],\n",
      "       [1],\n",
      "       [2],\n",
      "       [0],\n",
      "       [2],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [0],\n",
      "       [2],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [2],\n",
      "       [0],\n",
      "       [1],\n",
      "       [2],\n",
      "       [0]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "next_batch = input_pipeline(FILE_TRAIN, True) # Will return 32 random elements\n",
    "\n",
    "#Test our pipeline\n",
    "with tf.Session() as sess:\n",
    "    first_batch = sess.run(next_batch)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the feature_columns, which specifies the input to our model\n",
    "# All our input features are numeric, so use numeric_column for each one\n",
    "#https://www.tensorflow.org/versions/master/api_docs/python/tf/feature_column\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature_col = [tf.feature_column.numeric_column(k) for k in feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/shishir/Documents/botconnect Playground/TF1.3_input_pipeline/ModelStats', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "#create a DNN regression classifier, using the DNNClassifier \n",
    "#pre-made estimator\n",
    "#https://www.tensorflow.org/versions/master/api_docs/python/tf/estimator/DNNClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "estimator = tf.estimator.DNNClassifier(\n",
    "    feature_columns = feature_col, #feed input features to the model\n",
    "    hidden_units = [10, 10], # 2 layers, each with 10 nuerons\n",
    "    n_classes = 3,\n",
    "    model_dir = \"/Users/shishir/Documents/botconnect Playground/TF1.3_input_pipeline/ModelStats\") #PATH where checkpoints, model, tb graph etc is stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /Users/shishir/Documents/botconnect Playground/TF1.3_input_pipeline/ModelStats/model.ckpt.\n",
      "INFO:tensorflow:loss = 52.255, step = 1\n",
      "INFO:tensorflow:global_step/sec: 420.069\n",
      "INFO:tensorflow:loss = 6.00833, step = 101 (0.239 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 113 into /Users/shishir/Documents/botconnect Playground/TF1.3_input_pipeline/ModelStats/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.37632.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x1a1fbcacf8>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train our model using input_pipeline\n",
    "# Input to training is a file with training example\n",
    "# Stop training after 8 iterations of train data (epochs)\n",
    "# Estimators require an input_fn with no arguments, so we createa function with no arguments \n",
    "# using lambda, which calls our input_fn with the desired arguments: the file_path, shuffle \n",
    "# setting, and repeat_count. In our case, we use our new input_pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "estimator.train(\n",
    "    input_fn= lambda: input_pipeline(FILE_TRAIN, True, 30), steps = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-10-23-21:05:55\n",
      "INFO:tensorflow:Restoring parameters from /Users/shishir/Documents/botconnect Playground/TF1.3_input_pipeline/ModelStats/model.ckpt-113\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-23-21:05:56\n",
      "INFO:tensorflow:Saving dict for global step 113: accuracy = 0.966667, average_loss = 0.155541, global_step = 113, loss = 4.66622\n",
      "Evaluation results\n",
      "   accuracy, was: 0.9666666388511658\n",
      "   average_loss, was: 0.15554076433181763\n",
      "   loss, was: 4.666223049163818\n",
      "   global_step, was: 113\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our model using the examples contained in FILE_TEST\n",
    "# Return value will contain evaluation_metrics such as: loss & average_loss\n",
    "\n",
    "\n",
    "\n",
    "evaluate_result = estimator.evaluate(\n",
    "    input_fn=lambda: input_pipeline(FILE_TEST, False, 4))\n",
    "print(\"Evaluation results\")\n",
    "for key in evaluate_result:\n",
    "    print(\"   {}, was: {}\".format(key, evaluate_result[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test data\n",
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from /Users/shishir/Documents/botconnect Playground/TF1.3_input_pipeline/ModelStats/model.ckpt-113\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Predicting different iris flowers in testfile csv\n",
    "\n",
    "\n",
    "\n",
    "predict_results = estimator.predict(\n",
    "    input_fn=lambda: input_pipeline(FILE_TEST, False, 1))\n",
    "print(\"Predictions on test data\")\n",
    "for p in predict_results:\n",
    "    #Will classify 0,1 or 2 if Sentosa, Vericolor, Verginia resp.\n",
    "    print(p[\"class_ids\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let create a dataset for prediction\n",
    "# We've taken the first 3 examples in FILE_TEST\n",
    "\n",
    "\n",
    "\n",
    "prediction_input = [[5.9, 3.0, 4.2, 1.5],  # -> 1, Iris Versicolor\n",
    "                    [6.9, 3.1, 5.4, 2.1],  # -> 2, Iris Virginica\n",
    "                    [5.1, 3.3, 1.7, 0.5]]  # -> 0, Iris Sentosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_input_pipeline():\n",
    "    def decode(x):\n",
    "        x = tf.split(x, 4)  # Need to split into our 4 features\n",
    "        return dict(zip(feature_names, x))  # To build a dict of them\n",
    "\n",
    "    dataset = tf.contrib.data.Dataset.from_tensor_slices(prediction_input)\n",
    "    dataset = dataset.map(decode)\n",
    "    next_feature_batch = dataset.make_one_shot_iterator().get_next()\n",
    "    return next_feature_batch, None  # In prediction, we have no labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Predict all our prediction_input\n",
    "predict_results = estimator.predict(input_fn=test_input_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on memory\n",
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from /Users/shishir/Documents/botconnect Playground/TF1.3_input_pipeline/ModelStats/model.ckpt-113\n",
      "I think: [5.9, 3.0, 4.2, 1.5], is Iris Versicolor\n",
      "I think: [6.9, 3.1, 5.4, 2.1], is Iris Virginica\n",
      "I think: [5.1, 3.3, 1.7, 0.5], is Iris Sentosa\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"Predictions on memory\")\n",
    "for idx, prediction in enumerate(predict_results):\n",
    "    type = prediction[\"class_ids\"][0]  # Get the predicted class (index)\n",
    "    if type == 0:\n",
    "        print(\"{} - Iris Sentosa\".format(prediction_input[idx]))\n",
    "    elif type == 1:\n",
    "        print(\"{} - Iris Versicolor\".format(prediction_input[idx]))\n",
    "    else:\n",
    "        print(\"{} - Iris Virginica\".format(prediction_input[idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
